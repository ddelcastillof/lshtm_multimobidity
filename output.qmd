---
title: "An exercise in data cleaning and manipulation with EHR data"
author: "Darwin Del Castillo"
format:
  html:
    theme: default
    css: styles.css
    toc: true
    toc-depth: 1
    embed-resources: true
    scrollable: true
    controls-layout: bottom-right
    code-fold: true
    output-file: output.html
    output-dir: output/
---

```{r knitr-setup}
#| include: false
#| output: false
#| eval: true
#| echo: false
#| warning: false
#| message: false

knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      include = TRUE,
                      results = 'hide')
```

# Step 1: Loading datasets and data cleaning

For this analysis I used on the following packages:

```{r loading packages}
pacman::p_load(
  tidyverse,
  janitor,
  skimr,
  data.table,
  gtsummary,
  flextable)
```

`tidyverse` provides a suite for data manipulation and visualisation, `janitor` provides quick data cleaning utilities, `skimr` gives compact dataset diagnostics, `data.table` offers memory-efficient operations for EHR-scale workloads, and `gtsummary`/`flextable` produce tidy tables for presentation purposes.

Several steps apply `dplyr` functions to `data.table` objects, which coerces them to tibbles and can stress local memory on very large datasets. In real-world settings, I would use `dtplyr` to bypass the coercion to tibble, or use `data.table::merge()` directly. At the current dataset size the difference is modest, so I keep the current workflow for clarity.

# Step 2: Source code for data loading and cleaning

For loading the datasets, I used `data.table::fread()` function, which is optimized for loading large datasets. The code for loading and cleaning the datasets is as follows:

```{r}
#| file: "code/code_part_1.R"
```

After running the code above, the following datasets are available in the environment: `afib`, `pneumo`, `cases_medical`, `cases_personal`, and `cases_drugs`. Over these datasets, I performed data cleaning steps such as checking for duplicated patients, consistency between data types, and transforming dates to Date format using `lubridate`.

# Step 3: Data manipulation

For completing the processing steps, I created new variables using this code, such as age at diagnosis of aFib, and merged the datasets using `dplyr::left_join()` function or `dplyr. The code for these operations is as follows:

```{r}
#| file: "code/code_part_2.R"
```

# Step 4: Summary tables

Finally, I created summary tables using `gtsummary` and `flextable` packages to present the results of the analysis. The code for creating these tables is as follows:

```{r}
#| file: "code/code_part_3.R"
```

All the individual scripts are available in the [GitHub repository](https://github.com/ddelcastillof/lshtm_multimobidity).
